# ç¯å¢ƒæ£€æŸ¥
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path.cwd().parent
sys.path.insert(0, str(project_root / "src"))

# æ ¸å¿ƒåº“å¯¼å…¥
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm.notebook import tqdm
import warnings
import json
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ 
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report
)
from xgboost import XGBClassifier

# æ£€æŸ¥GPUå¯ç”¨æ€§
try:
    import torch
    gpu_available = torch.cuda.is_available()
    if gpu_available:
        print(f"âœ“ GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
    else:
        print("âš  GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
except ImportError:
    gpu_available = False
    print("âš  PyTorchæœªå®‰è£…ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")

# è®¾ç½®æ˜¾ç¤ºé€‰é¡¹
pd.set_option('display.max_columns', None)
plt.rcParams['figure.figsize'] = (12, 6)
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("âœ“ æ‰€æœ‰åº“å·²æˆåŠŸå¯¼å…¥")
print(f"âœ“ é¡¹ç›®æ ¹ç›®å½•: {project_root}")



def convert_numpy_types(obj):
    """é€’å½’è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
    import numpy as np
    if isinstance(obj, dict):
        return {k: convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(v) for v in obj]
    elif isinstance(obj, (np.integer, np.int64, np.int32)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64, np.float32)):
        return float(obj)
    elif isinstance(obj, (np.bool_, bool)):
        return bool(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    else:
        return obj


# ============== å‚æ•°é…ç½®åŒº ==============

CONFIG = {
    # è¾“å…¥è¾“å‡ºè·¯å¾„
    'processed_dir': project_root / 'data' / 'processed',
    'features_dir': project_root / 'outputs' / 'features',
    'cv_results_dir': project_root / 'outputs' / 'model_results' / 'phase3_binary' / 'cv_results',
    'feature_importance_dir': project_root / 'outputs' / 'model_results' / 'phase3_binary' / 'feature_importance',
    'transfer_results_dir': project_root / 'outputs' / 'model_results' / 'phase3_binary' / 'transfer_results',
    'figures_dir': project_root / 'outputs' / 'figures' / 'phase3',
    
    # æ¨¡å‹é€‰æ‹©ï¼ˆå¯é€‰: 'lr', 'rf', 'xgb'ï¼‰
    'models_to_train': ['lr', 'rf', 'xgb'],
    
    # äº¤å‰éªŒè¯å‚æ•°
    'n_folds': 5,
    'random_state': 42,
    
    # XGBoostå‚æ•°
    'use_gpu': gpu_available,
    'xgb_max_depth': 6,
    'xgb_learning_rate': 0.1,
    'xgb_n_estimators': 100,
    
    # Random Forestå‚æ•°
    'rf_n_estimators': 100,
    'rf_n_jobs': -1,
    
    # Logistic Regressionå‚æ•°
    'lr_max_iter': 1000,
    
    # å¯è§†åŒ–å‚æ•°
    'dpi': 300,
    'format': 'png',
    'display_plots': True,
    'max_display_plots': 8,
}

# åˆ›å»ºè¾“å‡ºç›®å½•
for key in ['cv_results_dir', 'feature_importance_dir', 'transfer_results_dir', 'figures_dir']:
    CONFIG[key].mkdir(parents=True, exist_ok=True)

print("é…ç½®å‚æ•°:")
print(f"  æ¨¡å‹: {CONFIG['models_to_train']}")
print(f"  äº¤å‰éªŒè¯æŠ˜æ•°: {CONFIG['n_folds']}")
print(f"  GPUåŠ é€Ÿ: {CONFIG['use_gpu']}")
print(f"  XGBoostå‚æ•°: max_depth={CONFIG['xgb_max_depth']}, lr={CONFIG['xgb_learning_rate']}")



def load_and_binarize_dataset(npz_path: Path, csv_path: Path, target: str):
    """
    åŠ è½½æ•°æ®å¹¶å°†æ ‡ç­¾äºŒå€¼åŒ–
    
    Args:
        npz_path: NPZç‰¹å¾æ–‡ä»¶
        csv_path: å¤„ç†åçš„CSVæ–‡ä»¶ï¼ˆåŒ…å«åˆ†é’Ÿæ ‡ç­¾ï¼‰
        target: 'SIF' or 'SGF'
    
    Returns:
        X, y_binary, median_threshold, feature_names
    """
    # åŠ è½½NPZç‰¹å¾
    data = np.load(npz_path, allow_pickle=True)
    X = data['X']
    feature_names = data['feature_names']
    ids_npz = data['ids']
    
    # åŠ è½½CSVè·å–åˆ†é’Ÿæ ‡ç­¾
    df = pd.read_csv(csv_path)
    df['id'] = df['id'].astype(str)
    
    # IDåŒ¹é…
    id_to_idx = {str(id_): idx for idx, id_ in enumerate(ids_npz)}
    valid_indices = []
    valid_labels = []
    
    label_col = f"{target}_minutes"
    for _, row in df.iterrows():
        row_id = str(row['id'])
        if row_id in id_to_idx:
            label = row[label_col]
            if label != -1 and not pd.isna(label):
                valid_indices.append(id_to_idx[row_id])
                valid_labels.append(label)
    
    # ç­›é€‰æœ‰æ•ˆæ ·æœ¬
    X_valid = X[valid_indices]
    y_minutes = np.array(valid_labels)
    
    # äºŒå€¼åŒ–ï¼šåŸºäºä¸­ä½æ•°
    median = np.median(y_minutes)
    y_binary = (y_minutes >= median).astype(int)  # 1=ç¨³å®š, 0=ä¸ç¨³å®š
    
    print(f"  æ ·æœ¬æ•°: {len(X_valid)}")
    print(f"  ä¸­ä½æ•°é˜ˆå€¼: {median:.1f} åˆ†é’Ÿ")
    print(f"  ç¨³å®š/ä¸ç¨³å®š: {np.sum(y_binary==1)}/{np.sum(y_binary==0)}")
    
    return X_valid, y_binary, median, feature_names

# åŠ è½½æ‰€æœ‰æ•°æ®é›†
datasets_data = {}
npz_files = sorted(CONFIG['features_dir'].glob('*_processed.npz'))

print(f"åŠ è½½å¹¶äºŒå€¼åŒ– {len(npz_files)} ä¸ªæ•°æ®é›†:\n")
for npz_file in npz_files:
    dataset_name = npz_file.stem.replace('_processed', '')
    csv_file = CONFIG['processed_dir'] / f"{dataset_name}_processed.csv"
    
    print(f"{dataset_name}:")
    
    # SIF
    X_sif, y_sif, median_sif, feat_names = load_and_binarize_dataset(npz_file, csv_file, 'SIF')
    print(f"  SIFå®Œæˆ")
    
    # SGF
    X_sgf, y_sgf, median_sgf, _ = load_and_binarize_dataset(npz_file, csv_file, 'SGF')
    print(f"  SGFå®Œæˆ\n")
    
    datasets_data[dataset_name] = {
        'X_sif': X_sif,
        'y_sif': y_sif,
        'median_sif': median_sif,
        'X_sgf': X_sgf,
        'y_sgf': y_sgf,
        'median_sgf': median_sgf,
        'feature_names': feat_names,
    }

print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆï¼å…± {len(datasets_data)} ä¸ªæ•°æ®é›†")



def get_model(model_name: str, use_gpu: bool = False):
    """
    åˆ›å»ºæ¨¡å‹å®ä¾‹
    """
    if model_name == 'lr':
        return LogisticRegression(
            max_iter=CONFIG['lr_max_iter'],
            class_weight='balanced',
            random_state=CONFIG['random_state']
        )
    elif model_name == 'rf':
        return RandomForestClassifier(
            n_estimators=CONFIG['rf_n_estimators'],
            class_weight='balanced',
            n_jobs=CONFIG['rf_n_jobs'],
            random_state=CONFIG['random_state']
        )
    elif model_name == 'xgb':
        params = {
            'max_depth': CONFIG['xgb_max_depth'],
            'learning_rate': CONFIG['xgb_learning_rate'],
            'n_estimators': CONFIG['xgb_n_estimators'],
            'random_state': CONFIG['random_state'],
            'tree_method': 'hist',
        }
        if use_gpu:
            params['device'] = 'cuda:0'
        return XGBClassifier(**params)
    else:
        raise ValueError(f"Unknown model: {model_name}")

def cross_validate_model(X, y, model_name: str, dataset_name: str, target: str):
    """
    æ‰§è¡ŒkæŠ˜äº¤å‰éªŒè¯
    
    Returns:
        dict: CVç»“æœ
    """
    # è‡ªåŠ¨è°ƒæ•´foldæ•°ï¼ˆå°æ•°æ®é›†ï¼‰
    min_class_count = np.bincount(y).min()
    n_folds = min(CONFIG['n_folds'], min_class_count)
    if n_folds < CONFIG['n_folds']:
        print(f"    âš  æ ·æœ¬æ•°è¾ƒå°‘ï¼Œè°ƒæ•´foldæ•°ä¸º {n_folds}")
    
    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=CONFIG['random_state'])
    
    metrics = {
        'accuracy': [],
        'precision': [],
        'recall': [],
        'f1': [],
        'auc': []
    }
    
    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]
        
        # è®­ç»ƒæ¨¡å‹
        model = get_model(model_name, CONFIG['use_gpu'])
        model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred = model.predict(X_test)
        y_proba = model.predict_proba(X_test)[:, 1]
        
        # è®¡ç®—æŒ‡æ ‡
        metrics['accuracy'].append(accuracy_score(y_test, y_pred))
        metrics['precision'].append(precision_score(y_test, y_pred, average='binary', zero_division=0))
        metrics['recall'].append(recall_score(y_test, y_pred, average='binary', zero_division=0))
        metrics['f1'].append(f1_score(y_test, y_pred, average='binary', zero_division=0))
        
        # AUCï¼ˆéœ€è¦è‡³å°‘ä¸¤ä¸ªç±»åˆ«ï¼‰
        if len(np.unique(y_test)) > 1:
            metrics['auc'].append(roc_auc_score(y_test, y_proba))
        else:
            metrics['auc'].append(np.nan)
    
    # æ±‡æ€»ç»“æœ
    results = {
        'dataset': dataset_name,
        'target': target,
        'model': model_name,
        'n_folds': n_folds,
        'metrics': metrics,
        'mean_metrics': {k: np.nanmean(v) for k, v in metrics.items()},
        'std_metrics': {k: np.nanstd(v) for k, v in metrics.items()},
    }
    
    return results

# æ‰§è¡Œï¼šæ‰¹é‡äº¤å‰éªŒè¯
print("å¼€å§‹äº¤å‰éªŒè¯è®­ç»ƒ...\n")
cv_results_all = []

for dataset_name, data in tqdm(datasets_data.items(), desc="æ•°æ®é›†"):
    print(f"\n{dataset_name}:")
    
    for target in ['SIF', 'SGF']:
        X = data[f'X_{target.lower()}']
        y = data[f'y_{target.lower()}']
        
        if len(y) == 0:
            print(f"  {target}: æ— æœ‰æ•ˆæ ·æœ¬ï¼Œè·³è¿‡")
            continue
        
        print(f"  {target}:")
        for model_name in CONFIG['models_to_train']:
            print(f"    {model_name.upper()}...", end=" ")
            results = cross_validate_model(X, y, model_name, dataset_name, target)
            cv_results_all.append(results)
            
            # ä¿å­˜ç»“æœ
            result_path = CONFIG['cv_results_dir'] / f"{dataset_name}_{target}_{model_name}_cv.json"
            with open(result_path, 'w') as f:
                json.dump(convert_numpy_types(results), f, indent=2)
            
            print(f"F1={results['mean_metrics']['f1']:.4f} âœ“")

print(f"\nâœ“ äº¤å‰éªŒè¯å®Œæˆï¼å…± {len(cv_results_all)} ä¸ªå®éªŒ")
print(f"  ç»“æœå·²ä¿å­˜åˆ°: {CONFIG['cv_results_dir'].relative_to(project_root)}")


# æå–RFå’ŒXGBoostçš„ç‰¹å¾é‡è¦æ€§ï¼ˆåªå¯¹ä¸€ä¸ªæ•°æ®é›†ç¤ºä¾‹ï¼‰
print("æå–ç‰¹å¾é‡è¦æ€§...\n")

importance_count = 0
for dataset_name, data in datasets_data.items():
    for target in ['SIF', 'SGF']:
        X = data[f'X_{target.lower()}']
        y = data[f'y_{target.lower()}']
        feature_names = data['feature_names']
        
        if len(y) == 0:
            continue
        
        # RFç‰¹å¾é‡è¦æ€§
        rf = get_model('rf', False)
        rf.fit(X, y)
        
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': rf.feature_importances_
        }).sort_values('importance', ascending=False)
        
        imp_path = CONFIG['feature_importance_dir'] / f"{dataset_name}_{target}_rf_importance.csv"
        importance_df.to_csv(imp_path, index=False)
        importance_count += 1
        
        # XGBoostç‰¹å¾é‡è¦æ€§
        xgb = get_model('xgb', CONFIG['use_gpu'])
        xgb.fit(X, y)
        
        importance_df_xgb = pd.DataFrame({
            'feature': feature_names,
            'importance': xgb.feature_importances_
        }).sort_values('importance', ascending=False)
        
        imp_path_xgb = CONFIG['feature_importance_dir'] / f"{dataset_name}_{target}_xgb_importance.csv"
        importance_df_xgb.to_csv(imp_path_xgb, index=False)
        importance_count += 1

print(f"âœ“ ç‰¹å¾é‡è¦æ€§æå–å®Œæˆï¼å…± {importance_count} ä¸ªæ–‡ä»¶")
print(f"  ç»“æœå·²ä¿å­˜åˆ°: {CONFIG['feature_importance_dir'].relative_to(project_root)}")


def transfer_learning_test(X_train, y_train, X_test, y_test, model_name: str, use_gpu: bool = False):
    """
    è¿ç§»å­¦ä¹ æµ‹è¯•
    """
    model = get_model(model_name, use_gpu)
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    
    metrics = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred, average='binary', zero_division=0),
        'recall': recall_score(y_test, y_pred, average='binary', zero_division=0),
        'f1': f1_score(y_test, y_pred, average='binary', zero_division=0),
        'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),
    }
    
    return metrics

# æ‰§è¡Œï¼šåŒå‘è¿ç§»å­¦ä¹ 
print("å¼€å§‹è¿ç§»å­¦ä¹ æµ‹è¯•...\n")
transfer_results_all = []
dataset_names = list(datasets_data.keys())

for i, train_dataset in enumerate(dataset_names):
    for j, test_dataset in enumerate(dataset_names):
        if i == j:  # è·³è¿‡è‡ªå·±
            continue
        
        print(f"{train_dataset} â†’ {test_dataset}:")
        
        for target in ['SIF', 'SGF']:
            X_train = datasets_data[train_dataset][f'X_{target.lower()}']
            y_train = datasets_data[train_dataset][f'y_{target.lower()}']
            X_test = datasets_data[test_dataset][f'X_{target.lower()}']
            y_test = datasets_data[test_dataset][f'y_{target.lower()}']
            
            if len(y_train) == 0 or len(y_test) == 0:
                print(f"  {target}: æ ·æœ¬ä¸è¶³ï¼Œè·³è¿‡")
                continue
            
            print(f"  {target}:", end=" ")
            for model_name in CONFIG['models_to_train']:
                metrics = transfer_learning_test(X_train, y_train, X_test, y_test, model_name, CONFIG['use_gpu'])
                
                result = {
                    'train_dataset': train_dataset,
                    'test_dataset': test_dataset,
                    'target': target,
                    'model': model_name,
                    'metrics': metrics,
                }
                transfer_results_all.append(result)
                
                # ä¿å­˜ç»“æœ
                result_path = CONFIG['transfer_results_dir'] / f"{train_dataset}_to_{test_dataset}_{target}_{model_name}.json"
                with open(result_path, 'w') as f:
                    json.dump(result, f, indent=2)
                
                print(f"{model_name.upper()}(F1={metrics['f1']:.3f})", end=" ")
            print()

print(f"\nâœ“ è¿ç§»å­¦ä¹ å®Œæˆï¼å…± {len(transfer_results_all)} ä¸ªå®éªŒ")
print(f"  ç»“æœå·²ä¿å­˜åˆ°: {CONFIG['transfer_results_dir'].relative_to(project_root)}")



print("="*70)
print("Phase 3: æ¨¡å‹éªŒè¯ - æ‰§è¡Œå®Œæ¯•")
print("="*70)

print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
print(f"\n  1. CVç»“æœ ({len(list(CONFIG['cv_results_dir'].glob('*.json')))} ä¸ªJSON):")
print(f"     {CONFIG['cv_results_dir'].relative_to(project_root)}")

print(f"\n  2. ç‰¹å¾é‡è¦æ€§ ({len(list(CONFIG['feature_importance_dir'].glob('*.csv')))} ä¸ªCSV):")
print(f"     {CONFIG['feature_importance_dir'].relative_to(project_root)}")

print(f"\n  3. è¿ç§»å­¦ä¹ ç»“æœ ({len(list(CONFIG['transfer_results_dir'].glob('*.json')))} ä¸ªJSON):")
print(f"     {CONFIG['transfer_results_dir'].relative_to(project_root)}")

print(f"\n  4. å¯è§†åŒ–å›¾è¡¨ ({len(list(CONFIG['figures_dir'].glob('*.png')))} ä¸ªPNG):")
for f in sorted(CONFIG['figures_dir'].glob('*.png')):
    print(f"     - {f.name}")

# æ‰¾å‡ºæœ€ä½³æ¨¡å‹
best_cv = max(cv_results_all, key=lambda x: x['mean_metrics']['f1'])
print("\nğŸ† æœ€ä½³CVæ¨¡å‹:")
print(f"  æ•°æ®é›†: {best_cv['dataset']}")
print(f"  ç›®æ ‡: {best_cv['target']}")
print(f"  æ¨¡å‹: {best_cv['model'].upper()}")
print(f"  F1 Score: {best_cv['mean_metrics']['f1']:.4f} Â± {best_cv['std_metrics']['f1']:.4f}")
print(f"  Accuracy: {best_cv['mean_metrics']['accuracy']:.4f} Â± {best_cv['std_metrics']['accuracy']:.4f}")

best_transfer = max(transfer_results_all, key=lambda x: x['metrics']['f1'])
print("\nğŸŒ æœ€ä½³è¿ç§»å­¦ä¹ æ¨¡å‹:")
print(f"  è®­ç»ƒé›†: {best_transfer['train_dataset']}")
print(f"  æµ‹è¯•é›†: {best_transfer['test_dataset']}")
print(f"  ç›®æ ‡: {best_transfer['target']}")
print(f"  æ¨¡å‹: {best_transfer['model'].upper()}")
print(f"  F1 Score: {best_transfer['metrics']['f1']:.4f}")

print("\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
print(f"  äº¤å‰éªŒè¯å®éªŒ: {len(cv_results_all)}")
print(f"  è¿ç§»å­¦ä¹ å®éªŒ: {len(transfer_results_all)}")
print(f"  ç‰¹å¾é‡è¦æ€§åˆ†æ: {importance_count}")

print("\nâœ… Phase 3 å®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜")
print("="*70)