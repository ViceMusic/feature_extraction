{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: æ¨¡å‹éªŒè¯ (Model Validation)\n",
    "\n",
    "## æ¦‚è¿° Overview\n",
    "\n",
    "æœ¬Notebookå®ç°Phase 3çš„äºŒåˆ†ç±»å»ºæ¨¡ä¸è¿ç§»å­¦ä¹ éªŒè¯ã€‚\n",
    "\n",
    "**ä¸»è¦ä»»åŠ¡ï¼š**\n",
    "1. **æ ‡ç­¾äºŒå€¼åŒ–**: åŸºäºä¸­ä½æ•°å°†ç¨³å®šæ€§åˆ†ä¸º\"ç¨³å®š\"vs\"ä¸ç¨³å®š\"\n",
    "2. **äº¤å‰éªŒè¯**: 5æŠ˜åˆ†å±‚CVï¼Œè®­ç»ƒLR/RF/XGBoostä¸‰ç§æ¨¡å‹\n",
    "3. **ç‰¹å¾é‡è¦æ€§**: åˆ†ææœ€å…·é¢„æµ‹æ€§çš„åˆ†å­ç‰¹å¾\n",
    "4. **è¿ç§»å­¦ä¹ **: è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†é—´çš„æ³›åŒ–èƒ½åŠ›\n",
    "5. **ç»“æœå¯è§†åŒ–**: æ··æ·†çŸ©é˜µã€æ€§èƒ½å¯¹æ¯”ã€è¿ç§»çƒ­åŠ›å›¾\n",
    "\n",
    "**æ¨¡å‹é…ç½®ï¼š**\n",
    "- Logistic Regression: L2æ­£åˆ™åŒ–, balancedæƒé‡\n",
    "- Random Forest: 100æ£µæ ‘, balancedæƒé‡\n",
    "- XGBoost: GPUåŠ é€Ÿ, max_depth=6, balancedæƒé‡\n",
    "\n",
    "**è¾“å…¥**: \n",
    "- `data/processed/*.csv` - å¸¦åˆ†é’Ÿæ ‡ç­¾çš„CSV  \n",
    "- `outputs/features/*.npz` - RDKitç‰¹å¾çŸ©é˜µ  \n",
    "\n",
    "**è¾“å‡º**: \n",
    "- `outputs/model_results/phase3_binary/cv_results/` - CVç»“æœJSON  \n",
    "- `outputs/model_results/phase3_binary/feature_importance/` - ç‰¹å¾é‡è¦æ€§CSV  \n",
    "- `outputs/model_results/phase3_binary/transfer_results/` - è¿ç§»å­¦ä¹ JSON  \n",
    "- `outputs/figures/phase3/` - å¯è§†åŒ–å›¾è¡¨  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒæ£€æŸ¥ä¸å¯¼å…¥ Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¯å¢ƒæ£€æŸ¥\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "# æ ¸å¿ƒåº“å¯¼å…¥\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# æœºå™¨å­¦ä¹ \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# æ£€æŸ¥GPUå¯ç”¨æ€§\n",
    "try:\n",
    "    import torch\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    if gpu_available:\n",
    "        print(f\"âœ“ GPUå¯ç”¨: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"âš  GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ\")\n",
    "except ImportError:\n",
    "    gpu_available = False\n",
    "    print(\"âš  PyTorchæœªå®‰è£…ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ\")\n",
    "\n",
    "# è®¾ç½®æ˜¾ç¤ºé€‰é¡¹\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ æ‰€æœ‰åº“å·²æˆåŠŸå¯¼å…¥\")\n",
    "print(f\"âœ“ é¡¹ç›®æ ¹ç›®å½•: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\ndef convert_numpy_types(obj):\n    \"\"\"é€’å½’è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹\"\"\"\n    import numpy as np\n    if isinstance(obj, dict):\n        return {k: convert_numpy_types(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [convert_numpy_types(v) for v in obj]\n    elif isinstance(obj, (np.integer, np.int64, np.int32)):\n        return int(obj)\n    elif isinstance(obj, (np.floating, np.float64, np.float32)):\n        return float(obj)\n    elif isinstance(obj, (np.bool_, bool)):\n        return bool(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    else:\n        return obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å‚æ•°é…ç½®åŒº Configuration\n",
    "\n",
    "**âš™ï¸ æ ¹æ®æ‚¨çš„éœ€æ±‚ä¿®æ”¹ä»¥ä¸‹å‚æ•°**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== å‚æ•°é…ç½®åŒº ==============\n",
    "\n",
    "CONFIG = {\n",
    "    # è¾“å…¥è¾“å‡ºè·¯å¾„\n",
    "    'processed_dir': project_root / 'data' / 'processed',\n",
    "    'features_dir': project_root / 'outputs' / 'features',\n",
    "    'cv_results_dir': project_root / 'outputs' / 'model_results' / 'phase3_binary' / 'cv_results',\n",
    "    'feature_importance_dir': project_root / 'outputs' / 'model_results' / 'phase3_binary' / 'feature_importance',\n",
    "    'transfer_results_dir': project_root / 'outputs' / 'model_results' / 'phase3_binary' / 'transfer_results',\n",
    "    'figures_dir': project_root / 'outputs' / 'figures' / 'phase3',\n",
    "    \n",
    "    # æ¨¡å‹é€‰æ‹©ï¼ˆå¯é€‰: 'lr', 'rf', 'xgb'ï¼‰\n",
    "    'models_to_train': ['lr', 'rf', 'xgb'],\n",
    "    \n",
    "    # äº¤å‰éªŒè¯å‚æ•°\n",
    "    'n_folds': 5,\n",
    "    'random_state': 42,\n",
    "    \n",
    "    # XGBoostå‚æ•°\n",
    "    'use_gpu': gpu_available,\n",
    "    'xgb_max_depth': 6,\n",
    "    'xgb_learning_rate': 0.1,\n",
    "    'xgb_n_estimators': 100,\n",
    "    \n",
    "    # Random Forestå‚æ•°\n",
    "    'rf_n_estimators': 100,\n",
    "    'rf_n_jobs': -1,\n",
    "    \n",
    "    # Logistic Regressionå‚æ•°\n",
    "    'lr_max_iter': 1000,\n",
    "    \n",
    "    # å¯è§†åŒ–å‚æ•°\n",
    "    'dpi': 300,\n",
    "    'format': 'png',\n",
    "    'display_plots': True,\n",
    "    'max_display_plots': 8,\n",
    "}\n",
    "\n",
    "# åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "for key in ['cv_results_dir', 'feature_importance_dir', 'transfer_results_dir', 'figures_dir']:\n",
    "    CONFIG[key].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"é…ç½®å‚æ•°:\")\n",
    "print(f\"  æ¨¡å‹: {CONFIG['models_to_train']}\")\n",
    "print(f\"  äº¤å‰éªŒè¯æŠ˜æ•°: {CONFIG['n_folds']}\")\n",
    "print(f\"  GPUåŠ é€Ÿ: {CONFIG['use_gpu']}\")\n",
    "print(f\"  XGBoostå‚æ•°: max_depth={CONFIG['xgb_max_depth']}, lr={CONFIG['xgb_learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ•°æ®åŠ è½½ä¸äºŒå€¼åŒ– Data Loading & Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_binarize_dataset(npz_path: Path, csv_path: Path, target: str):\n",
    "    \"\"\"\n",
    "    åŠ è½½æ•°æ®å¹¶å°†æ ‡ç­¾äºŒå€¼åŒ–\n",
    "    \n",
    "    Args:\n",
    "        npz_path: NPZç‰¹å¾æ–‡ä»¶\n",
    "        csv_path: å¤„ç†åçš„CSVæ–‡ä»¶ï¼ˆåŒ…å«åˆ†é’Ÿæ ‡ç­¾ï¼‰\n",
    "        target: 'SIF' or 'SGF'\n",
    "    \n",
    "    Returns:\n",
    "        X, y_binary, median_threshold, feature_names\n",
    "    \"\"\"\n",
    "    # åŠ è½½NPZç‰¹å¾\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "    X = data['X']\n",
    "    feature_names = data['feature_names']\n",
    "    ids_npz = data['ids']\n",
    "    \n",
    "    # åŠ è½½CSVè·å–åˆ†é’Ÿæ ‡ç­¾\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['id'] = df['id'].astype(str)\n",
    "    \n",
    "    # IDåŒ¹é…\n",
    "    id_to_idx = {str(id_): idx for idx, id_ in enumerate(ids_npz)}\n",
    "    valid_indices = []\n",
    "    valid_labels = []\n",
    "    \n",
    "    label_col = f\"{target}_minutes\"\n",
    "    for _, row in df.iterrows():\n",
    "        row_id = str(row['id'])\n",
    "        if row_id in id_to_idx:\n",
    "            label = row[label_col]\n",
    "            if label != -1 and not pd.isna(label):\n",
    "                valid_indices.append(id_to_idx[row_id])\n",
    "                valid_labels.append(label)\n",
    "    \n",
    "    # ç­›é€‰æœ‰æ•ˆæ ·æœ¬\n",
    "    X_valid = X[valid_indices]\n",
    "    y_minutes = np.array(valid_labels)\n",
    "    \n",
    "    # äºŒå€¼åŒ–ï¼šåŸºäºä¸­ä½æ•°\n",
    "    median = np.median(y_minutes)\n",
    "    y_binary = (y_minutes >= median).astype(int)  # 1=ç¨³å®š, 0=ä¸ç¨³å®š\n",
    "    \n",
    "    print(f\"  æ ·æœ¬æ•°: {len(X_valid)}\")\n",
    "    print(f\"  ä¸­ä½æ•°é˜ˆå€¼: {median:.1f} åˆ†é’Ÿ\")\n",
    "    print(f\"  ç¨³å®š/ä¸ç¨³å®š: {np.sum(y_binary==1)}/{np.sum(y_binary==0)}\")\n",
    "    \n",
    "    return X_valid, y_binary, median, feature_names\n",
    "\n",
    "# åŠ è½½æ‰€æœ‰æ•°æ®é›†\n",
    "datasets_data = {}\n",
    "npz_files = sorted(CONFIG['features_dir'].glob('*_processed.npz'))\n",
    "\n",
    "print(f\"åŠ è½½å¹¶äºŒå€¼åŒ– {len(npz_files)} ä¸ªæ•°æ®é›†:\\n\")\n",
    "for npz_file in npz_files:\n",
    "    dataset_name = npz_file.stem.replace('_processed', '')\n",
    "    csv_file = CONFIG['processed_dir'] / f\"{dataset_name}_processed.csv\"\n",
    "    \n",
    "    print(f\"{dataset_name}:\")\n",
    "    \n",
    "    # SIF\n",
    "    X_sif, y_sif, median_sif, feat_names = load_and_binarize_dataset(npz_file, csv_file, 'SIF')\n",
    "    print(f\"  SIFå®Œæˆ\")\n",
    "    \n",
    "    # SGF\n",
    "    X_sgf, y_sgf, median_sgf, _ = load_and_binarize_dataset(npz_file, csv_file, 'SGF')\n",
    "    print(f\"  SGFå®Œæˆ\\n\")\n",
    "    \n",
    "    datasets_data[dataset_name] = {\n",
    "        'X_sif': X_sif,\n",
    "        'y_sif': y_sif,\n",
    "        'median_sif': median_sif,\n",
    "        'X_sgf': X_sgf,\n",
    "        'y_sgf': y_sgf,\n",
    "        'median_sgf': median_sgf,\n",
    "        'feature_names': feat_names,\n",
    "    }\n",
    "\n",
    "print(f\"âœ“ æ•°æ®åŠ è½½å®Œæˆï¼å…± {len(datasets_data)} ä¸ªæ•°æ®é›†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. äº¤å‰éªŒè¯è®­ç»ƒ Cross-Validation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name: str, use_gpu: bool = False):\n",
    "    \"\"\"\n",
    "    åˆ›å»ºæ¨¡å‹å®ä¾‹\n",
    "    \"\"\"\n",
    "    if model_name == 'lr':\n",
    "        return LogisticRegression(\n",
    "            max_iter=CONFIG['lr_max_iter'],\n",
    "            class_weight='balanced',\n",
    "            random_state=CONFIG['random_state']\n",
    "        )\n",
    "    elif model_name == 'rf':\n",
    "        return RandomForestClassifier(\n",
    "            n_estimators=CONFIG['rf_n_estimators'],\n",
    "            class_weight='balanced',\n",
    "            n_jobs=CONFIG['rf_n_jobs'],\n",
    "            random_state=CONFIG['random_state']\n",
    "        )\n",
    "    elif model_name == 'xgb':\n",
    "        params = {\n",
    "            'max_depth': CONFIG['xgb_max_depth'],\n",
    "            'learning_rate': CONFIG['xgb_learning_rate'],\n",
    "            'n_estimators': CONFIG['xgb_n_estimators'],\n",
    "            'random_state': CONFIG['random_state'],\n",
    "            'tree_method': 'hist',\n",
    "        }\n",
    "        if use_gpu:\n",
    "            params['device'] = 'cuda:0'\n",
    "        return XGBClassifier(**params)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "def cross_validate_model(X, y, model_name: str, dataset_name: str, target: str):\n",
    "    \"\"\"\n",
    "    æ‰§è¡ŒkæŠ˜äº¤å‰éªŒè¯\n",
    "    \n",
    "    Returns:\n",
    "        dict: CVç»“æœ\n",
    "    \"\"\"\n",
    "    # è‡ªåŠ¨è°ƒæ•´foldæ•°ï¼ˆå°æ•°æ®é›†ï¼‰\n",
    "    min_class_count = np.bincount(y).min()\n    n_folds = min(CONFIG['n_folds'], min_class_count)\n",
    "    if n_folds < CONFIG['n_folds']:\n",
    "        print(f\"    âš  æ ·æœ¬æ•°è¾ƒå°‘ï¼Œè°ƒæ•´foldæ•°ä¸º {n_folds}\")\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=CONFIG['random_state'])\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'auc': []\n",
    "    }\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # è®­ç»ƒæ¨¡å‹\n",
    "        model = get_model(model_name, CONFIG['use_gpu'])\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # é¢„æµ‹\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # è®¡ç®—æŒ‡æ ‡\n",
    "        metrics['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "        metrics['precision'].append(precision_score(y_test, y_pred, average='binary', zero_division=0))\n",
    "        metrics['recall'].append(recall_score(y_test, y_pred, average='binary', zero_division=0))\n",
    "        metrics['f1'].append(f1_score(y_test, y_pred, average='binary', zero_division=0))\n",
    "        \n",
    "        # AUCï¼ˆéœ€è¦è‡³å°‘ä¸¤ä¸ªç±»åˆ«ï¼‰\n",
    "        if len(np.unique(y_test)) > 1:\n",
    "            metrics['auc'].append(roc_auc_score(y_test, y_proba))\n",
    "        else:\n",
    "            metrics['auc'].append(np.nan)\n",
    "    \n",
    "    # æ±‡æ€»ç»“æœ\n",
    "    results = {\n",
    "        'dataset': dataset_name,\n",
    "        'target': target,\n",
    "        'model': model_name,\n",
    "        'n_folds': n_folds,\n",
    "        'metrics': metrics,\n",
    "        'mean_metrics': {k: np.nanmean(v) for k, v in metrics.items()},\n",
    "        'std_metrics': {k: np.nanstd(v) for k, v in metrics.items()},\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# æ‰§è¡Œï¼šæ‰¹é‡äº¤å‰éªŒè¯\n",
    "print(\"å¼€å§‹äº¤å‰éªŒè¯è®­ç»ƒ...\\n\")\n",
    "cv_results_all = []\n",
    "\n",
    "for dataset_name, data in tqdm(datasets_data.items(), desc=\"æ•°æ®é›†\"):\n",
    "    print(f\"\\n{dataset_name}:\")\n",
    "    \n",
    "    for target in ['SIF', 'SGF']:\n",
    "        X = data[f'X_{target.lower()}']\n",
    "        y = data[f'y_{target.lower()}']\n",
    "        \n",
    "        if len(y) == 0:\n",
    "            print(f\"  {target}: æ— æœ‰æ•ˆæ ·æœ¬ï¼Œè·³è¿‡\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  {target}:\")\n",
    "        for model_name in CONFIG['models_to_train']:\n",
    "            print(f\"    {model_name.upper()}...\", end=\" \")\n",
    "            results = cross_validate_model(X, y, model_name, dataset_name, target)\n",
    "            cv_results_all.append(results)\n",
    "            \n",
    "            # ä¿å­˜ç»“æœ\n",
    "            result_path = CONFIG['cv_results_dir'] / f\"{dataset_name}_{target}_{model_name}_cv.json\"\n",
    "            with open(result_path, 'w') as f:\n",
    "                json.dump(convert_numpy_types(results), f, indent=2)\n",
    "            \n",
    "            print(f\"F1={results['mean_metrics']['f1']:.4f} âœ“\")\n",
    "\n",
    "print(f\"\\nâœ“ äº¤å‰éªŒè¯å®Œæˆï¼å…± {len(cv_results_all)} ä¸ªå®éªŒ\")\n",
    "print(f\"  ç»“æœå·²ä¿å­˜åˆ°: {CONFIG['cv_results_dir'].relative_to(project_root)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ç‰¹å¾é‡è¦æ€§åˆ†æ Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå–RFå’ŒXGBoostçš„ç‰¹å¾é‡è¦æ€§ï¼ˆåªå¯¹ä¸€ä¸ªæ•°æ®é›†ç¤ºä¾‹ï¼‰\n",
    "print(\"æå–ç‰¹å¾é‡è¦æ€§...\\n\")\n",
    "\n",
    "importance_count = 0\n",
    "for dataset_name, data in datasets_data.items():\n",
    "    for target in ['SIF', 'SGF']:\n",
    "        X = data[f'X_{target.lower()}']\n",
    "        y = data[f'y_{target.lower()}']\n",
    "        feature_names = data['feature_names']\n",
    "        \n",
    "        if len(y) == 0:\n",
    "            continue\n",
    "        \n",
    "        # RFç‰¹å¾é‡è¦æ€§\n",
    "        rf = get_model('rf', False)\n",
    "        rf.fit(X, y)\n",
    "        \n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': rf.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        imp_path = CONFIG['feature_importance_dir'] / f\"{dataset_name}_{target}_rf_importance.csv\"\n",
    "        importance_df.to_csv(imp_path, index=False)\n",
    "        importance_count += 1\n",
    "        \n",
    "        # XGBoostç‰¹å¾é‡è¦æ€§\n",
    "        xgb = get_model('xgb', CONFIG['use_gpu'])\n",
    "        xgb.fit(X, y)\n",
    "        \n",
    "        importance_df_xgb = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': xgb.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        imp_path_xgb = CONFIG['feature_importance_dir'] / f\"{dataset_name}_{target}_xgb_importance.csv\"\n",
    "        importance_df_xgb.to_csv(imp_path_xgb, index=False)\n",
    "        importance_count += 1\n",
    "\n",
    "print(f\"âœ“ ç‰¹å¾é‡è¦æ€§æå–å®Œæˆï¼å…± {importance_count} ä¸ªæ–‡ä»¶\")\n",
    "print(f\"  ç»“æœå·²ä¿å­˜åˆ°: {CONFIG['feature_importance_dir'].relative_to(project_root)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. è¿ç§»å­¦ä¹  Transfer Learning\n",
    "\n",
    "è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†é—´çš„æ³›åŒ–èƒ½åŠ›ï¼ˆTrain on A, Test on Bï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_learning_test(X_train, y_train, X_test, y_test, model_name: str, use_gpu: bool = False):\n",
    "    \"\"\"\n",
    "    è¿ç§»å­¦ä¹ æµ‹è¯•\n",
    "    \"\"\"\n",
    "    model = get_model(model_name, use_gpu)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, average='binary', zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, average='binary', zero_division=0),\n",
    "        'f1': f1_score(y_test, y_pred, average='binary', zero_division=0),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# æ‰§è¡Œï¼šåŒå‘è¿ç§»å­¦ä¹ \n",
    "print(\"å¼€å§‹è¿ç§»å­¦ä¹ æµ‹è¯•...\\n\")\n",
    "transfer_results_all = []\n",
    "dataset_names = list(datasets_data.keys())\n",
    "\n",
    "for i, train_dataset in enumerate(dataset_names):\n",
    "    for j, test_dataset in enumerate(dataset_names):\n",
    "        if i == j:  # è·³è¿‡è‡ªå·±\n",
    "            continue\n",
    "        \n",
    "        print(f\"{train_dataset} â†’ {test_dataset}:\")\n",
    "        \n",
    "        for target in ['SIF', 'SGF']:\n",
    "            X_train = datasets_data[train_dataset][f'X_{target.lower()}']\n",
    "            y_train = datasets_data[train_dataset][f'y_{target.lower()}']\n",
    "            X_test = datasets_data[test_dataset][f'X_{target.lower()}']\n",
    "            y_test = datasets_data[test_dataset][f'y_{target.lower()}']\n",
    "            \n",
    "            if len(y_train) == 0 or len(y_test) == 0:\n",
    "                print(f\"  {target}: æ ·æœ¬ä¸è¶³ï¼Œè·³è¿‡\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  {target}:\", end=\" \")\n",
    "            for model_name in CONFIG['models_to_train']:\n",
    "                metrics = transfer_learning_test(X_train, y_train, X_test, y_test, model_name, CONFIG['use_gpu'])\n",
    "                \n",
    "                result = {\n",
    "                    'train_dataset': train_dataset,\n",
    "                    'test_dataset': test_dataset,\n",
    "                    'target': target,\n",
    "                    'model': model_name,\n",
    "                    'metrics': metrics,\n",
    "                }\n",
    "                transfer_results_all.append(result)\n",
    "                \n",
    "                # ä¿å­˜ç»“æœ\n",
    "                result_path = CONFIG['transfer_results_dir'] / f\"{train_dataset}_to_{test_dataset}_{target}_{model_name}.json\"\n",
    "                with open(result_path, 'w') as f:\n",
    "                    json.dump(result, f, indent=2)\n",
    "                \n",
    "                print(f\"{model_name.upper()}(F1={metrics['f1']:.3f})\", end=\" \")\n",
    "            print()\n",
    "\n",
    "print(f\"\\nâœ“ è¿ç§»å­¦ä¹ å®Œæˆï¼å…± {len(transfer_results_all)} ä¸ªå®éªŒ\")\n",
    "print(f\"  ç»“æœå·²ä¿å­˜åˆ°: {CONFIG['transfer_results_dir'].relative_to(project_root)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ç»“æœå¯è§†åŒ– Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 CVæ€§èƒ½å¯¹æ¯”å›¾\n",
    "print(\"ç”ŸæˆCVæ€§èƒ½å¯¹æ¯”å›¾...\")\n",
    "\n",
    "cv_df = pd.DataFrame([{\n",
    "    'dataset': r['dataset'],\n",
    "    'target': r['target'],\n",
    "    'model': r['model'].upper(),\n",
    "    'f1': r['mean_metrics']['f1'],\n",
    "    'accuracy': r['mean_metrics']['accuracy'],\n",
    "} for r in cv_results_all])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# SIF\n",
    "sif_df = cv_df[cv_df['target'] == 'SIF'].pivot(index='dataset', columns='model', values='f1')\n",
    "sif_df.plot(kind='bar', ax=axes[0], width=0.8)\n",
    "axes[0].set_title('SIF - äº¤å‰éªŒè¯F1 Scoreå¯¹æ¯”', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('F1 Score', fontsize=11)\n",
    "axes[0].set_xlabel('æ•°æ®é›†', fontsize=11)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].legend(title='Model')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# SGF\n",
    "sgf_df = cv_df[cv_df['target'] == 'SGF'].pivot(index='dataset', columns='model', values='f1')\n",
    "sgf_df.plot(kind='bar', ax=axes[1], width=0.8)\n",
    "axes[1].set_title('SGF - äº¤å‰éªŒè¯F1 Scoreå¯¹æ¯”', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylabel('F1 Score', fontsize=11)\n",
    "axes[1].set_xlabel('æ•°æ®é›†', fontsize=11)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].legend(title='Model')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "cv_plot_path = CONFIG['figures_dir'] / 'cv_performance_comparison.png'\n",
    "plt.savefig(cv_plot_path, dpi=CONFIG['dpi'], bbox_inches='tight')\n",
    "print(f\"âœ“ å·²ä¿å­˜: {cv_plot_path.name}\")\n",
    "\n",
    "if CONFIG['display_plots']:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 è¿ç§»å­¦ä¹ çƒ­åŠ›å›¾ï¼ˆç¤ºä¾‹ï¼šRFæ¨¡å‹ï¼ŒSIFç›®æ ‡ï¼‰\n",
    "print(\"\\nç”Ÿæˆè¿ç§»å­¦ä¹ çƒ­åŠ›å›¾...\")\n",
    "\n",
    "for target in ['SIF', 'SGF']:\n",
    "    for model in ['lr', 'rf', 'xgb']:\n",
    "        # æ„å»ºF1 ScoreçŸ©é˜µ\n",
    "        transfer_df = pd.DataFrame([{\n",
    "            'train': r['train_dataset'],\n",
    "            'test': r['test_dataset'],\n",
    "            'f1': r['metrics']['f1']\n",
    "        } for r in transfer_results_all if r['target'] == target and r['model'] == model])\n",
    "        \n",
    "        if len(transfer_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        heatmap_data = transfer_df.pivot(index='train', columns='test', values='f1')\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "                   vmin=0, vmax=1, ax=ax, cbar_kws={'label': 'F1 Score'})\n",
    "        ax.set_title(f'{target} - {model.upper()} è¿ç§»å­¦ä¹ æ€§èƒ½\\n(è¡Œ=è®­ç»ƒé›†, åˆ—=æµ‹è¯•é›†)', \n",
    "                    fontsize=13, fontweight='bold')\n",
    "        ax.set_xlabel('æµ‹è¯•æ•°æ®é›†', fontsize=11)\n",
    "        ax.set_ylabel('è®­ç»ƒæ•°æ®é›†', fontsize=11)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        heatmap_path = CONFIG['figures_dir'] / f'transfer_heatmap_{target}_{model}.png'\n",
    "        plt.savefig(heatmap_path, dpi=CONFIG['dpi'], bbox_inches='tight')\n",
    "        print(f\"âœ“ å·²ä¿å­˜: {heatmap_path.name}\")\n",
    "        \n",
    "        if CONFIG['display_plots']:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ç»“æœæ€»ç»“ Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Phase 3: æ¨¡å‹éªŒè¯ - æ‰§è¡Œå®Œæ¯•\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:\")\n",
    "print(f\"\\n  1. CVç»“æœ ({len(list(CONFIG['cv_results_dir'].glob('*.json')))} ä¸ªJSON):\")\n",
    "print(f\"     {CONFIG['cv_results_dir'].relative_to(project_root)}\")\n",
    "\n",
    "print(f\"\\n  2. ç‰¹å¾é‡è¦æ€§ ({len(list(CONFIG['feature_importance_dir'].glob('*.csv')))} ä¸ªCSV):\")\n",
    "print(f\"     {CONFIG['feature_importance_dir'].relative_to(project_root)}\")\n",
    "\n",
    "print(f\"\\n  3. è¿ç§»å­¦ä¹ ç»“æœ ({len(list(CONFIG['transfer_results_dir'].glob('*.json')))} ä¸ªJSON):\")\n",
    "print(f\"     {CONFIG['transfer_results_dir'].relative_to(project_root)}\")\n",
    "\n",
    "print(f\"\\n  4. å¯è§†åŒ–å›¾è¡¨ ({len(list(CONFIG['figures_dir'].glob('*.png')))} ä¸ªPNG):\")\n",
    "for f in sorted(CONFIG['figures_dir'].glob('*.png')):\n",
    "    print(f\"     - {f.name}\")\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³æ¨¡å‹\n",
    "best_cv = max(cv_results_all, key=lambda x: x['mean_metrics']['f1'])\n",
    "print(\"\\nğŸ† æœ€ä½³CVæ¨¡å‹:\")\n",
    "print(f\"  æ•°æ®é›†: {best_cv['dataset']}\")\n",
    "print(f\"  ç›®æ ‡: {best_cv['target']}\")\n",
    "print(f\"  æ¨¡å‹: {best_cv['model'].upper()}\")\n",
    "print(f\"  F1 Score: {best_cv['mean_metrics']['f1']:.4f} Â± {best_cv['std_metrics']['f1']:.4f}\")\n",
    "print(f\"  Accuracy: {best_cv['mean_metrics']['accuracy']:.4f} Â± {best_cv['std_metrics']['accuracy']:.4f}\")\n",
    "\n",
    "best_transfer = max(transfer_results_all, key=lambda x: x['metrics']['f1'])\n",
    "print(\"\\nğŸŒ æœ€ä½³è¿ç§»å­¦ä¹ æ¨¡å‹:\")\n",
    "print(f\"  è®­ç»ƒé›†: {best_transfer['train_dataset']}\")\n",
    "print(f\"  æµ‹è¯•é›†: {best_transfer['test_dataset']}\")\n",
    "print(f\"  ç›®æ ‡: {best_transfer['target']}\")\n",
    "print(f\"  æ¨¡å‹: {best_transfer['model'].upper()}\")\n",
    "print(f\"  F1 Score: {best_transfer['metrics']['f1']:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ“Š æ€»ä½“ç»Ÿè®¡:\")\n",
    "print(f\"  äº¤å‰éªŒè¯å®éªŒ: {len(cv_results_all)}\")\n",
    "print(f\"  è¿ç§»å­¦ä¹ å®éªŒ: {len(transfer_results_all)}\")\n",
    "print(f\"  ç‰¹å¾é‡è¦æ€§åˆ†æ: {importance_count}\")\n",
    "\n",
    "print(\"\\nâœ… Phase 3 å®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}