{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf1ece9",
   "metadata": {},
   "source": [
    "# 读取npy数据为某种格式\n",
    "\n",
    "## 1.该notebook脚本说明\n",
    "* 读取内容为/npy_datas中的npy文件（由上一个脚本生成，每个数据集对应生成一个）\n",
    "\n",
    "### 1.1前置条件\n",
    "如上所述\n",
    "\n",
    "### 1.2notebook功能区域划分\n",
    "* cell-1：加载所需的库以及配置\n",
    "* cell-2：加载读取所需函数\n",
    "* cell-3：输出测试以及验证结果可行性\n",
    "\n",
    "### 1.3脚本执行目标\n",
    "输入\n",
    "```\n",
    "    根文件路径，\n",
    "    数据集名称，\n",
    "    任务:\"SIF\"/\"SGF\"\n",
    "```\n",
    "输出：\n",
    "```\n",
    "    筛选后的X矩阵\n",
    "    筛选后的Y（原始值）\n",
    "    筛选后的Y（根据阈值二值化）\n",
    "    特征名称列表\n",
    "```\n",
    "\n",
    "⚠ **Notice**：\n",
    "* 在本文件开始之前，在前置notebook文件中已经进行了monomer的筛选，该文件读到的所有npy文件均是单体的信息\n",
    "* 阈值，异常值可以在cell-1中设置\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7f30663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------该cell是一些测试的东西，可以忽略掉--------------------------------- \n",
    "\n",
    "\n",
    "# 测试内容如下\n",
    "# 需要做的东西包括：读取npy文件，根据SIF和SGF标签进行二值化筛选\n",
    "# 参数：根文件路径，筛选数据集，任务类型（SIF/SGF）\n",
    "# 中间需要的筛选逻辑：\n",
    "    # 1. 读取X.npy, y_sif.npy, y_sgf.npy, ids.npy\n",
    "    # 2. 根据is_monomer和任务类型，筛选出对应的样本\n",
    "    # 3. 对y进行二值化处理（根据SIF和SGF进行二值化处理）\n",
    "    # 4. 输出筛选后的X和y\n",
    "# 输出：筛选后的X和被二值化的y\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# 除此之外，我比较喜欢写注释而不是markdown\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "#   script_dir = Path(__file__).parent  # 脚本目录,但是这个在notebook里面不能用\n",
    "#   script_dir = Path().resolve()  # 当前工作目录\n",
    "#   data_path = script_dir / \"../outputs/npy_datas/sif_sgf_second_processed/X.npy\"\n",
    "#   X = np.load(data_path)\n",
    "#   print(X.shape)\n",
    "\n",
    "#   feature_names_path = script_dir / \"../outputs/npy_datas/sif_sgf_second_processed/feature_names.npy\"\n",
    "#   feature_names = np.load(feature_names_path, allow_pickle=True).tolist()\n",
    "#   print(len(feature_names))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 测试了一下，Morgan分子指纹应该把分子值转化为numpy里面的布尔数值了\n",
    "# 合理猜测，Aovlan也是一样的处理方式\n",
    "# 注意feature_names里面的列名是字符串类型的，并且是Morgan之类的，而不是原本csv里面的列名，所以到了这一步基本就无法读取是否为单体分子了\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a4a9d",
   "metadata": {},
   "source": [
    "## 2.Cell-1：导入数据以及配置相关库文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c868031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据成功\n",
      "判断所有数据均存在...\n",
      "sif_sgf_second 的npy数据文件存在。\n",
      "US9624268 的npy数据文件存在。\n",
      "US9809623B2 的npy数据文件存在。\n",
      "US20140294902A1 的npy数据文件存在。\n",
      "WO2017011820A2 的npy数据文件存在。\n",
      "所有数据文件均存在。\n"
     ]
    }
   ],
   "source": [
    "# 库函数\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "CONFIG = {\n",
    "    'npy_datas_dir': Path().resolve() / 'npy_datas',\n",
    "    'dataset_names': ['sif_sgf_second', 'US9624268', 'US9809623B2','US20140294902A1', 'WO2017011820A2'],\n",
    "    'dataset': {\n",
    "        'sif_sgf_second': 'sif_sgf_second_processed',\n",
    "        'US9624268': 'US9624268_processed',\n",
    "        'US9809623B2': 'US9809623B2_processed',\n",
    "        'US20140294902A1': 'US20140294902A1_processed',\n",
    "        'WO2017011820A2': 'WO2017011820A2_processed',\n",
    "    },\n",
    "    'abnormal_point':700,\n",
    "    'threshold'   : {\n",
    "        'sif': 270,\n",
    "        'sgf': 250,\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "print(\"加载数据成功\")\n",
    "print(\"判断所有数据均存在...\")\n",
    "for dataset_name in CONFIG['dataset_names']:\n",
    "    dataset_dir = CONFIG['npy_datas_dir'] / CONFIG['dataset'][dataset_name]\n",
    "    required_files = ['X.npy', 'y_sif.npy', 'y_sgf.npy', 'feature_names.npy']\n",
    "    for file_name in required_files:\n",
    "        file_path = dataset_dir / file_name\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"缺少文件: {file_path}\")\n",
    "    print(f\"{dataset_name} 的npy数据文件存在。\")\n",
    "print(\"所有数据文件均存在。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c1c8f1",
   "metadata": {},
   "source": [
    "## 3.Cell-2：筛选方法生成数据和信息\n",
    "\n",
    "最后获得结果为ndarray数据格式，其中包括X，原始y值，二值化y值，特征名称\n",
    "* 1. 获得数据默认做了如下处理：去除所有非单体，去掉所有对应任务半衰期（y值）为nan数据，去掉与当前任务无关的数据\n",
    "* 2. 该方法获得数据的意义为：在【某数据集】中所有可用于【某任务】的，满足【非异常值】【非单体】和【半衰期非nan】的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc352ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 该方法的返回值为筛选后的X（特定数据集，特定的任务，以及异常值筛选），二值化后的y（根据特定任务进行阈值划分），以及特征名称列表\n",
    "\n",
    "def load_and_filter_from_npy(\n",
    "    root_dir: Path,         # 数据集的根文件路径，这里只要outputs/npy_datas就行\n",
    "    dataset_name: str,      # 数据集的名称即可，_processed之类的后缀不需要,代码里面会加上的\n",
    "    target: str             # \"SIF\" or \"SGF\"，大写与否都无所谓\n",
    "):\n",
    "    \"\"\"\n",
    "    从 NPY 文件中读取数据，根据任务类型和 is_monomer 进行筛选，\n",
    "    并对标签进行二值化处理。\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_filtered : np.ndarray, shape (M, D)\n",
    "    y_binary   : np.ndarray, shape (M,)\n",
    "    feature_names : np.ndarray or list\n",
    "    \"\"\"\n",
    "    # 1.读取数据\n",
    "    dataset_dir = root_dir / dataset_name\n",
    "    X = np.load(dataset_dir / \"X.npy\")\n",
    "    y_sif = np.load(dataset_dir / \"y_sif.npy\")\n",
    "    y_sgf = np.load(dataset_dir / \"y_sgf.npy\")\n",
    "    feature_names = np.load(dataset_dir / \"feature_names.npy\", allow_pickle=True)\n",
    "\n",
    "    # 一致性检查，如果不通过就报错\n",
    "    n = X.shape[0]\n",
    "    assert y_sif.shape[0] == n\n",
    "    assert y_sgf.shape[0] == n\n",
    "\n",
    "    # 2. 找到 is_monomer 在 X 中对应的列号-----这个后面根据序号进行扩充吧,该方法已经被完全废弃，因为筛选已经移动到csv中\n",
    "    #feature_names = list(feature_names)\n",
    "    #if \"is_monomer\" not in feature_names:\n",
    "    #    raise ValueError(\"feature_names 中未找到 'is_monomer' 特征\")\n",
    "    #monomer_col_idx = feature_names.index(\"is_monomer\")\n",
    "\n",
    "    y_raw = None\n",
    "    threshold = None\n",
    "    abnormal_point = CONFIG['abnormal_point']\n",
    "\n",
    "    # 3. 选择任务标签\n",
    "    if target.upper() == \"SIF\":\n",
    "        y_raw = y_sif\n",
    "        threshold = CONFIG['threshold']['sif']\n",
    "    elif target.upper() == \"SGF\":\n",
    "        y_raw = y_sgf\n",
    "        threshold = CONFIG['threshold']['sgf']\n",
    "    else:\n",
    "        raise ValueError(\"target 必须是 'SIF' 或 'SGF'\")\n",
    "\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 4和5用了三个很有意思的特性， csv转npy时候的自动转化操作，numpy的筛选操作，多重叠加的按位与操作\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    # 4. 构造样本筛选 mask\n",
    "    valid_mask = np.ones(n, dtype=bool)  # 生成的原址数值全都为1\n",
    "\n",
    "    # 4.1 根据 is_monomer 过滤（如果指定），该方法已经被完全废弃，因为筛选已经移动到csv中\n",
    "    #if is_monomer is not None:\n",
    "    #    valid_mask &= (X[:, monomer_col_idx] == int(is_monomer)) # 如果是单体，则筛选出单体样本，否则筛选出非单体样本\n",
    "\n",
    "    # 4.2 过滤无效标签(mask是按位与操作)\n",
    "    valid_mask &= (y_raw != -1)   # 添加筛选条件，若为-1则过滤\n",
    "    valid_mask &= ~np.isnan(y_raw) # 添加筛选条件，如果是nan则过滤掉\n",
    "    valid_mask &= (y_raw <= abnormal_point)  # 添加筛选条件，若大于异常值则过滤掉\n",
    "\n",
    "    # 5. 应用筛选（按位置删除）\n",
    "    X_valid = X[valid_mask]\n",
    "    y_valid = y_raw[valid_mask]\n",
    "\n",
    "    print(f\"筛选后样本数: {X_valid.shape[0]} / {X.shape[0]}\")\n",
    "\n",
    "    # 6. 二值化标签，半衰期大于阈值则视为稳定，否则视为不稳定\n",
    "    y_binary = (y_valid >= threshold).astype(np.int32)\n",
    "\n",
    "    print(\"============================================================================\")\n",
    "    print(f\"根据任务{target}筛选数据集 {dataset_name} 后的统计信息:\")\n",
    "    print(f\"  样本数: {X_valid.shape[0]}\")\n",
    "    print(f\"  本次筛选设定阈值为: {threshold} 分钟，异常值为{abnormal_point}分钟\")\n",
    "    print(f\"  稳定/不稳定: {(y_binary == 1).sum()} / {(y_binary == 0).sum()}\")\n",
    "    print(f\"    符合该条件的数据条数：{X_valid.shape[0]}，特征维度: {X_valid.shape[1]}\")\n",
    "    print(f\"    特征名称示例: {feature_names[:5]} ...\")\n",
    "    print(f\"    {target}半衰期范围: {y_valid.min()} - {y_valid.max()}\")\n",
    "    print(\"============================================================================\")\n",
    "\n",
    "    return X_valid,y_valid, y_binary, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db800a2c",
   "metadata": {},
   "source": [
    "## 4.cell-3：代码测试以及方法使用说明\n",
    "\n",
    "大致使用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7049aa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "筛选后样本数: 130 / 130\n",
      "============================================================================\n",
      "根据任务SIF筛选数据集 US9624268_processed 后的统计信息:\n",
      "  样本数: 130\n",
      "  本次筛选设定阈值为: 270 分钟，异常值为700分钟\n",
      "  稳定/不稳定: 76 / 54\n",
      "    符合该条件的数据条数：130，特征维度: 536\n",
      "    特征名称示例: ['QED_MW' 'QED_ALOGP' 'QED_HBA' 'QED_HBD' 'QED_PSA'] ...\n",
      "    SIF半衰期范围: 30 - 420\n",
      "============================================================================\n",
      "筛选后数据形状和类型:\n",
      "(130, 536) (130,) (130,) 536\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "### 方法读取示例\n",
    "X_filtered,y_filtered, y_binary, feature_names = load_and_filter_from_npy(\n",
    "    root_dir=CONFIG['npy_datas_dir'],   \n",
    "    dataset_name=CONFIG['dataset']['US9624268'],\n",
    "    target='SIF'\n",
    ")\n",
    "\n",
    "\n",
    "print(\"筛选后数据形状和类型:\")\n",
    "print(X_filtered.shape,y_filtered.shape, y_binary.shape, len(feature_names))\n",
    "print(type(X_filtered),type(y_filtered), type(y_binary), type(feature_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feature-extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
